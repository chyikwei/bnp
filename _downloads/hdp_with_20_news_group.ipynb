{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "HDP with 20 news group data\n\nModified form scikit-learn's \"Topic extraction with NMF and LDA\"\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Author: Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n\nfrom __future__ import print_function\nfrom time import time\n\nimport numpy as np\nfrom numpy.random import RandomState\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.utils import shuffle\n\nfrom bnp.online_hdp import HierarchicalDirichletProcess\n\nn_iter = 5\nn_features = 1000\nn_topic_truncate = 50\nn_doc_truncate = 10\nn_top_words = 10\nn_top_topics = 5\nn_inference_docs = 20\n\nrs = RandomState(100)\n\n\ndef print_top_words(model, feature_names, n_top_words):\n    topic_distr = model.topic_distribution()\n    for topic_idx in range(model.lambda_.shape[0]):\n        topic = model.lambda_[topic_idx, :]\n        message = \"Topic #%d (%.3f): \" % (topic_idx, topic_distr[topic_idx])\n        topics_idx = topic.argsort()[:-n_top_words - 1:-1]\n        features = [feature_names[t] for t in topics_idx]\n        message += \" \".join(features)\n        print(message)\n    print()\n\n\n# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n# to filter out useless terms early on: the posts are stripped of headers,\n# footers and quoted replies, and common English words, words occurring in\n# only one document or in at least 95% of the documents are removed.\n\nprint(\"Loading dataset...\")\nt0 = time()\ndataset = fetch_20newsgroups(shuffle=True, random_state=1,\n                             remove=('headers', 'footers', 'quotes'))\ntarget_names = dataset.target_names\ntrain_samples = dataset.data[:-n_inference_docs]\ntrain_targets = dataset.target[:-n_inference_docs]\ninference_samples = dataset.data[-n_inference_docs:]\ninference_targets = dataset.target[-n_inference_docs:]\nprint(\"done in %0.3fs.\" % (time() - t0))\n\n# Use tf (raw term count) features for HDP.\nprint(\"Extracting tf features for HDP...\")\ntf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n                                max_features=n_features,\n                                stop_words='english')\nt0 = time()\ntf = tf_vectorizer.fit_transform(train_samples)\nprint(\"done in %0.3fs.\" % (time() - t0))\nprint()\n\nprint(\"Fitting HDP models with tf features, \"\n      \"n_samples=%d and n_features=%d...\"\n      % (tf.shape[0], n_features))\nhdp = HierarchicalDirichletProcess(n_topic_truncate=n_topic_truncate,\n                                   n_doc_truncate=n_doc_truncate,\n                                   omega=2.0,\n                                   alpha=1.0,\n                                   kappa=0.7,\n                                   tau=64.,\n                                   max_iter=10,\n                                   learning_method='online',\n                                   batch_size=250,\n                                   total_samples=1e6,\n                                   max_doc_update_iter=200,\n                                   verbose=1,\n                                   mean_change_tol=1e-3,\n                                   random_state=100)\n\nfor i in range(5):\n    t0 = time()\n    print(\"iter %d\" % i)\n    suffled_tf = shuffle(tf, random_state=rs)\n    hdp.partial_fit(suffled_tf)\n    print(\"done in %0.3fs.\" % (time() - t0))\n\nprint(\"\\nTopics in HDP model:\")\ntf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(hdp, tf_feature_names, n_top_words)\n\n# top topics in each group\nprint(\"\\nTop topics in each group:\")\ntrain_topics = hdp.transform(tf)\n# normalize\ntrain_topics = train_topics / np.sum(train_topics, axis=1)[:, np.newaxis]\nfor grp_idx, group_name in enumerate(target_names):\n    doc_idx = np.where(train_targets == grp_idx)[0]\n    mean_doc_topics = np.mean(train_topics[doc_idx, :], axis=0)\n    top_idx = mean_doc_topics.argsort()[:-n_top_topics - 1:-1]\n    print(\"group: %s:\" % group_name)\n    print(\"top topics: %s\" % (\", \".join([\"#%d (%.3f)\" %\n          (idx, mean_doc_topics[idx]) for idx in top_idx])))\n    print()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.11", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}